{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "#! pip install keras==2.4.0\n",
        "#! pip install tensorflow==2.4.1\n",
        "\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm_notebook"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hex Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Styles:\n",
        "    WHITE = (255, 255, 255)\n",
        "    RED = (255,0,0,255)\n",
        "    BLUE = (0,0,255,255)\n",
        "    BLACK = (0,0,0,255)\n",
        "    GREEN = (0,255,0,255)\n",
        "    \n",
        "    PLAYER1 = RED\n",
        "    PLAYER2 = BLUE\n",
        "    EMPTY = BLACK\n",
        "\n",
        "    # cell value corresponds to index in GAME_COLORS\n",
        "    GAME_COLORS = [EMPTY, PLAYER1, PLAYER2]\n",
        "    BGCOLOR = WHITE\n",
        "\n",
        "    LINEWIDTH = 10\n",
        "    LINECOLOR = BLACK\n",
        "\n",
        "    CELLRADIUS = 50\n",
        "    #Must be minimum the 2xcellradius to prevent overlaps\n",
        "    CELLMARGIN = 2*CELLRADIUS + 100\n",
        "    #Must be minimum the cellradius to prevent grid from overflowing the image\n",
        "    IMAGEPADDING = CELLRADIUS + 20\n",
        "\n",
        "    GIF_FRAME_DELAY = 0.5\n",
        "\n",
        "\n",
        "class HexVisualizer:\n",
        "\n",
        "    def render_image(self, board, file_path):\n",
        "\n",
        "        grid_dimentions = (len(board) - 1) * Styles.CELLMARGIN\n",
        "        image_dimentions = grid_dimentions + 2 * Styles.IMAGEPADDING\n",
        "        image_center = image_dimentions / 2\n",
        "\n",
        "        # Calculate coordinates for all the board cells.\n",
        "        cell_coordinates = get_cell_coordinates(board, image_center)\n",
        "\n",
        "        image = create_image(image_dimentions, image_dimentions)\n",
        "        canvas = ImageDraw.Draw(image)\n",
        "\n",
        "        render_lines(cell_coordinates, canvas)\n",
        "        render_dots(cell_coordinates, canvas)\n",
        "\n",
        "        # rotate image -45deg to get the diamond shape\n",
        "        image = image.rotate(-45, Image.NEAREST, expand=1, fillcolor=Styles.WHITE)\n",
        "        # print(f'BOARD IMAGE SAVED AS {file_path}')\n",
        "        image.save(file_path)\n",
        "        image.close()\n",
        "\n",
        "        # Display image in Jupyter Notebook by using matplotlib.image\n",
        "        # img = mpimg.imread(file_path)\n",
        "        # imgplot = plt.imshow(img)\n",
        "        # imgplot.axes.get_xaxis().set_visible(False)\n",
        "        # imgplot.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "    def render_gif(self, state_history, file_path):\n",
        "        board_history = [s[0] for s in state_history]\n",
        "\n",
        "        images = []\n",
        "        for i, board in enumerate(board_history):\n",
        "            filename = f'out/tmp_img/img{i}.png'\n",
        "            hex_viz = HexVisualizer()\n",
        "            hex_viz.render_image(board, filename)\n",
        "            images.append(imageio.imread(filename))\n",
        "        imageio.mimsave(file_path, images, duration=Styles.GIF_FRAME_DELAY)\n",
        "        print(file_path)\n",
        "\n",
        "\n",
        "# returns a 2d-array where every grid row is an array of tuples\n",
        "# containing the node, x-, and y-coordinate: (the node, x, y)\n",
        "def get_cell_coordinates(board, image_center):\n",
        "    node_coordinates = []\n",
        "    for i in range(len(board)):\n",
        "        # y coordinate for current row\n",
        "        y = i * Styles.CELLMARGIN + Styles.IMAGEPADDING\n",
        "        row_width = (len(board[i]) - 1) * Styles.CELLMARGIN\n",
        "        row_start = image_center - (row_width / 2)\n",
        "\n",
        "        cell_coordinates_row = []\n",
        "        for j in range(len(board[i])):\n",
        "            # x coordinate for current dot\n",
        "            x = row_start + (j * Styles.CELLMARGIN)\n",
        "            cell_coordinates_row.append((board[i][j], x, y))\n",
        "        node_coordinates.append(cell_coordinates_row)\n",
        "\n",
        "    return node_coordinates\n",
        "\n",
        "def render_lines(cell_coordinates, canvas):\n",
        "    for i in range(len(cell_coordinates)):\n",
        "        for j in range(len(cell_coordinates[i])):\n",
        "            #render horizontal lines\n",
        "            if j+1 < len(cell_coordinates[i]):\n",
        "                color = Styles.LINECOLOR\n",
        "                #render PLAYER1 color for first and last horizontal line\n",
        "                if i == 0 or i == len(cell_coordinates)-1:\n",
        "                    color = Styles.PLAYER1\n",
        "                cell_connecting_line(canvas, cell_coordinates[i][j], cell_coordinates[i][j + 1], color)\n",
        "\n",
        "                #render diagonal lines\n",
        "                if i+1 < len(cell_coordinates):\n",
        "                    cell_connecting_line(canvas, cell_coordinates[i + 1][j], cell_coordinates[i][j + 1], Styles.LINECOLOR)\n",
        "            \n",
        "            #render vertical lines\n",
        "            if i+1 < len(cell_coordinates):\n",
        "                color = Styles.LINECOLOR\n",
        "                #render PLAYER2 color for first and last vertical line\n",
        "                if j == 0 or j == len(cell_coordinates[i])-1:\n",
        "                    color = Styles.PLAYER2\n",
        "                cell_connecting_line(canvas, cell_coordinates[i][j], cell_coordinates[i + 1][j], color)\n",
        "\n",
        "\n",
        "def render_dots(cell_coordinates, canvas):\n",
        "    for row in cell_coordinates:\n",
        "        for (cell, x, y) in row:\n",
        "            color = Styles.GAME_COLORS[cell]\n",
        "            draw_dot(canvas, [x, y], Styles.CELLRADIUS, color)            \n",
        "\n",
        "\n",
        "def create_image(width, height):\n",
        "    return Image.new(\"RGB\", (width, height), Styles.BGCOLOR)\n",
        "\n",
        "\n",
        "def draw_dot(drawing, center_coordinates, r, color, is_filled=True, width=1):\n",
        "    x, y = center_coordinates\n",
        "\n",
        "    upper_left_point = (x-r, y-r)\n",
        "    lower_right_point = (x+r, y+r)\n",
        "\n",
        "    # coordinates of upper left and lower right of circle/dot\n",
        "    coordinates = [upper_left_point, lower_right_point]\n",
        "\n",
        "    # possible to make a hollow dot by setting is_filled to false\n",
        "    if is_filled:\n",
        "        drawing.ellipse(coordinates, fill=color)\n",
        "    else:\n",
        "        drawing.ellipse(coordinates, fill=Styles.BGCOLOR, outline=color, width=width)\n",
        "\n",
        "\n",
        "def cell_connecting_line(drawing, cell1, cell2, color):\n",
        "    (_, x1, y1) = cell1\n",
        "    (_, x2, y2) = cell2\n",
        "    coordinates = [(x1, y1),(x2, y2)]\n",
        "    drawing.line(coordinates, fill=color, width=Styles.LINEWIDTH)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# State Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "class State_Manager:\n",
        "\n",
        "    def __init__(self, board_size):\n",
        "        self.board_size = board_size\n",
        "\n",
        "    def get_initial_board(self):\n",
        "        empty_board = np.zeros(shape=(self.board_size, self.board_size), dtype=int)\n",
        "        return empty_board\n",
        "\n",
        "    def get_legal_actions(self, state):\n",
        "        \"\"\"\n",
        "        return: a list of legal actions\n",
        "        \"\"\"\n",
        "        board, player = state\n",
        "        board_size = len(board)\n",
        "        actions = []    \n",
        "\n",
        "        for row in range(board_size):\n",
        "            for col in range(board_size):\n",
        "                if board[row][col] == 0:\n",
        "                    actions.append(((row, col), player))\n",
        "        return actions\n",
        "\n",
        "    def get_score(self, state) -> float:\n",
        "        \"\"\"\n",
        "        returns: the score of the state. If winning: +++, if loosing: ---\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        if self.check_if_winning(state):\n",
        "            _, next_player = state\n",
        "            player = change_player(next_player)\n",
        "            score = 1\n",
        "        else:\n",
        "            raise Exception('Game not scored')\n",
        "        return score\n",
        "\n",
        "\n",
        "    # action is a coordinate tuple of the cell to put the players piece: (row,col)\n",
        "    def get_child_state(self, state, action):\n",
        "        \"\"\"\n",
        "        paaram: state = tuple(game_board, player)\n",
        "        return: the child state that is reached when applying action to state\n",
        "\n",
        "        Gets a game board with player_id and coordinates for action\n",
        "        Returns a board with a peg put at that spot, and the player id for the next player\n",
        "        \"\"\"\n",
        "        board, _ = state\n",
        "        child_board = deepcopy(board)\n",
        "        # print(action)\n",
        "        (row, col), player = action\n",
        "        child_board[row][col] = player\n",
        "\n",
        "        return (child_board, change_player(player))\n",
        "\n",
        "    def check_if_winning(self, state):\n",
        "        board, next_player = state\n",
        "        player = change_player(next_player)\n",
        "        board_size = len(board)\n",
        "\n",
        "        # make the initial BFS queue for either player 1 or player 2\n",
        "        if player == 1:\n",
        "            queue = [(0, i) for i in range(board_size)]\n",
        "        elif player == 2:\n",
        "            queue = [(i, 0) for i in range(board_size)]\n",
        "        else:\n",
        "            raise Exception(\"player must be either 1 or 2\")\n",
        "\n",
        "        return bfs(state, queue)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Help Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# returns true if the player is in a winning state, else false\n",
        "def bfs(state, queue):\n",
        "    board, next_player = state\n",
        "    player = change_player(next_player)\n",
        "    # remove the nodes which doesn't contain player's piece from the initial queue\n",
        "    queue = [(row, col) for (row, col) in queue if board[row,col]==player]\n",
        "    # use collections.deque to get O(1) append and pop operations\n",
        "    queue = deque(queue)\n",
        "\n",
        "    board_size = len(board)\n",
        "    visited = []\n",
        "\n",
        "    while len(queue) > 0:\n",
        "        node = queue.popleft()\n",
        "        neighbors = get_neighbors(board, node)\n",
        "        for (row, col) in neighbors:\n",
        "            # all unvisited legal moves to a node containing one of the players pieces\n",
        "            if (row,col) not in visited and board[row][col] == player:\n",
        "                #check for win for player 1 and 2\n",
        "                if player == 1 and row == board_size-1:\n",
        "                    return True\n",
        "                elif player == 2 and col == board_size-1:\n",
        "                    return True\n",
        "                #if not win, append node to queue\n",
        "                queue.append((row, col))\n",
        "        visited.append(node)\n",
        "    return False        \n",
        "        \n",
        "\n",
        "# returns a list of all neighbouring coordinates from a given coordinate on the board\n",
        "def get_neighbors(board, coordinate):\n",
        "    legal_moves = []\n",
        "    board_size = len(board)\n",
        "    row, col = coordinate\n",
        "\n",
        "    if col < board_size-1:\n",
        "        # east\n",
        "        legal_moves.append((row,col+1))\n",
        "        if row > 0:\n",
        "            #north east\n",
        "            legal_moves.append((row-1,col+1))\n",
        "    if row > 0:\n",
        "        #north\n",
        "        legal_moves.append((row-1, col))\n",
        "    if col > 0:\n",
        "        #west\n",
        "        legal_moves.append((row, col-1))\n",
        "        if row < board_size-1:\n",
        "            #south west\n",
        "            legal_moves.append((row+1, col-1))\n",
        "    if row < board_size-1:\n",
        "        #south\n",
        "        legal_moves.append((row+1, col))\n",
        "    return legal_moves\n",
        "\n",
        "def change_player(player):\n",
        "    if player == 1:\n",
        "        return 2\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gameplay functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_game(sm, p1_policy, p2_policy):\n",
        "    board = sm.get_initial_board()\n",
        "    # Node.clear_memo()\n",
        "    player = 1\n",
        "    state = (board, player)\n",
        "\n",
        "    winner = None\n",
        "    node=None\n",
        "\n",
        "    move_counter = 0\n",
        "    state_history = []\n",
        "\n",
        "    while len(sm.get_legal_actions(state)) > 0 and winner is None:\n",
        "        move_counter += 1\n",
        "\n",
        "        state_history.append(state)\n",
        "        _, player = state\n",
        "        if player == 1:\n",
        "            move, root_node = p1_policy(state, node)\n",
        "            # print(f'p1 move: {move}')\n",
        "        else:\n",
        "            move, root_node = p2_policy(state, node)\n",
        "\n",
        "        child_state = sm.get_child_state(state, move)\n",
        "\n",
        "        node = root_node\n",
        "\n",
        "        if sm.check_if_winning(child_state):\n",
        "            winner = player\n",
        "            state_history.append(child_state)\n",
        "\n",
        "        # set state as the board state and the other player\n",
        "        state = child_state\n",
        "\n",
        "    winning_board, _ = state\n",
        "    return winner, state_history\n",
        "\n",
        "\n",
        "def random_player_move(state, root_node=None):\n",
        "    sm = State_Manager(4)\n",
        "    legal_actions = sm.get_legal_actions(state)\n",
        "    return random.choice(legal_actions), None\n",
        "\n",
        "def human_input_move(state, root_node=None):\n",
        "    curr_board, player = state\n",
        "    # HexVisualizer(curr_board, 'tmp.png')\n",
        "    print(curr_board)\n",
        "    print(state)\n",
        "    input_move = input('place piece at coordinate i.e: 2,0 >> ')\n",
        "    x,y = input_move.strip(' ').split(',')\n",
        "    move = ((int(x),int(y)), player)\n",
        "    return move, None\n",
        "\n",
        "# def mcts_player_move(state, root_node=None):\n",
        "#     mcts = MCTS(sm, 100, default_policy, uct, state, root_node)\n",
        "#     best_move, prob_distribution, child_node = mcts.choose_move()\n",
        "#     # print(\"\\n\\nState:\\n\", state, \"\\nPlayer:\", player, \"Best Move: \", best_move, \"\\nProb_dist:\\n\", np.round(prob_distribution, 3))\n",
        "#     return best_move, child_node\n",
        "\n",
        "\n",
        "# def play_multiple_games(n_games, p1_policy, p2_policy):\n",
        "#     history = []\n",
        "    \n",
        "#     for i in range(n_games):\n",
        "#         winner = play_game(sm, p1_policy, p2_policy)\n",
        "#         history.append(winner)\n",
        "\n",
        "#         p1_percent = 100*history.count(1)/len(history)\n",
        "#         p2_percent = 100*history.count(2)/len(history)\n",
        "\n",
        "#         print(f'GAME {i}: {winner} WINS  ==>  {round(p1_percent, 1)}% - {round(p2_percent, 1)}%')\n",
        "#     return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# play_game(State_Manager(4), random_player_move, human_input_move)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monte Carlo Tree Search\n",
        "## MCTS Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search:\n",
        "\n",
        "    Following pseudo code:\n",
        "    1. Traverse - Follow tree policy\n",
        "    2. Expand leaf\n",
        "    3. Rollout\n",
        "    4. Backpropagate\n",
        "    5. Choose best move\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_manager, n_sim, default_policy, utility_function, state, root_node=None, c=1.5):\n",
        "        \"\"\"\n",
        "        :param state_manager:    the state manager for the game\n",
        "        :param n_sim:            number of simulations\n",
        "        :param default_policy:   default policy for rollout       default_policy(state, action)\n",
        "        :param utility_function: used for scoring moves\n",
        "        :param state:            =(game_board, player)\n",
        "        :param root_node:        root_nfplode when tree is pruned\n",
        "        :param c:                exploratory constant for utility function\n",
        "\n",
        "        action = ((row, col), player)\n",
        "        \"\"\"\n",
        "        self.state_manager = state_manager\n",
        "        self.n_sim = n_sim\n",
        "        self.default_policy = default_policy\n",
        "        self.utility_function = utility_function\n",
        "        self.state = state\n",
        "        self.root_node = root_node\n",
        "        self.c = c\n",
        "\n",
        "    def choose_move(self):\n",
        "        \"\"\"\n",
        "        :return best_move, prob_distribution, child_node\n",
        "\n",
        "        Using the 4 steps of the MCTS to choose the best move. \n",
        "        \"\"\"\n",
        "        #Setting root_node\n",
        "        if self.root_node is None:\n",
        "            root_node = Node(self.state, self.state_manager)\n",
        "        else:\n",
        "            root_node = self.root_node\n",
        "            \n",
        "        #Performing n simulations of the MCTS tree\n",
        "        for i in range(self.n_sim):\n",
        "            \n",
        "            # 1. Traversing\n",
        "            #---------------\n",
        "            node_action_list = self.traverse(root_node)\n",
        "            \n",
        "            #Setting next_node as the node that will be expanded. In the initial state that is the root_node\n",
        "            if len(node_action_list) > 0:\n",
        "                last_node, last_action = node_action_list[-1]\n",
        "                next_node = last_node.children[last_action]\n",
        "            else:\n",
        "                next_node = root_node\n",
        "                \n",
        "            # 2. Expanding\n",
        "            #---------------\n",
        "            #Appends the expanded node an the action made at last node to node_action_list\n",
        "            #Returns nothing if node has no child states\n",
        "            node_action = self.expand(next_node)\n",
        "            if node_action is not None:\n",
        "                node_action_list.append(node_action)\n",
        "\n",
        "            # 3. Rolling out\n",
        "            #---------------\n",
        "            #Obtaining the score from the final state\n",
        "            if len(node_action_list)>0:\n",
        "                score = self.rollout(node_action_list[-1])\n",
        "            #if root_node is end state\n",
        "            else:\n",
        "                score = self.state_manager.get_score(self.state)\n",
        "                \n",
        "            \n",
        "            # 4. Backpropagating\n",
        "            #---------------\n",
        "            #Updating the score and visitcounts for all nodes and actions\n",
        "            self.back_propagate(node_action_list, score)\n",
        "            \n",
        "        # 5. Choosing the best move\n",
        "        #---------------\n",
        "        #Get the action with most visits\n",
        "        best_move = max(root_node.edges, key=(lambda k:root_node.edges[k][1]))\n",
        "        child_node = root_node.children[best_move]\n",
        "        \n",
        "        #Normalized probability distribution based on visit counts\n",
        "        prob_distribution = self.get_prob_distribution(root_node)\n",
        "        \n",
        "        return best_move, prob_distribution, child_node\n",
        "\n",
        "\n",
        "    def traverse(self, node):\n",
        "        \"\"\"\n",
        "        :param node: root node of the traversal\n",
        "        :return: List of tuples (node, action) in cronological order\n",
        "\n",
        "        From the root state (R) use the tree policy to choose the next pre-existing nodes\n",
        "        \"\"\"\n",
        "        node_action_list = []\n",
        "        \n",
        "        #Traversing down to the bottom of the existing tree and adding every step to the node_action_list\n",
        "        while len(node.children) > 0:\n",
        "            best_action = self.choose_action(node)\n",
        "            node_action_list.append((node, best_action))\n",
        "            node = node.get_child(best_action)\n",
        "    \n",
        "        return node_action_list\n",
        "    \n",
        "    def expand(self, node):\n",
        "        \"\"\"\n",
        "        :param node: root node of the traversal\n",
        "        :return: node, best_action\n",
        "\n",
        "        Generates children for the chosen node and returns the node along with the best action.\n",
        "        \"\"\"\n",
        "        \n",
        "        #No expansion if the node is an end state\n",
        "        if self.state_manager.check_if_winning(node.state):\n",
        "            return\n",
        "\n",
        "        #Expanding the node        \n",
        "        actions = self.state_manager.get_legal_actions(node.state)\n",
        "        node.set_child_states(actions, self.state_manager)\n",
        "\n",
        "        #Using the default policy to choose the best move from the expansion\n",
        "        #TODO: Check if works\n",
        "        best_action = self.default_policy.get_action(node.state, actions, stochastic=True)\n",
        "        \n",
        "        return node, best_action\n",
        "        \n",
        "    \n",
        "    def rollout(self, node_action):\n",
        "        \"\"\"\n",
        "        :param node_action: tuple(node, action)\n",
        "        :return score\n",
        "\n",
        "        Follow the default policy from the node in the node_action pair all the way to an end state\n",
        "        returning the score from the end state\n",
        "        \"\"\"\n",
        "        node, action = node_action\n",
        "        state = self.state_manager.get_child_state(node.state, action)\n",
        "\n",
        "        #If the incoming child_state is winning\n",
        "        if self.state_manager.check_if_winning(state):\n",
        "            score = self.state_manager.get_score(state)\n",
        "            return score\n",
        "        \n",
        "        #Else - using counter to get the correct value of the score (+/-)\n",
        "        winner = None\n",
        "        winning_state = None\n",
        "        counter = 0\n",
        "\n",
        "        #Rollout until winning state\n",
        "        while True:\n",
        "            if self.state_manager.check_if_winning(state):            \n",
        "                _, winner = state\n",
        "                winning_state = child_state\n",
        "                break  \n",
        "            \n",
        "            legal_actions = self.state_manager.get_legal_actions(state)\n",
        "            \n",
        "            #TODO: Check if works\n",
        "            best_action = self.default_policy.get_action(state, legal_actions, stochastic=True)\n",
        "            child_state = self.state_manager.get_child_state(state, best_action)\n",
        "\n",
        "            state = child_state\n",
        "            counter += 1\n",
        "        \n",
        "        #Adjusting the score depending on the number of steps until the root node\n",
        "        score = self.state_manager.get_score(winning_state)*(-1)**counter\n",
        "        return score\n",
        "\n",
        "\n",
        "    def back_propagate(self, node_action_list, score):\n",
        "        \"\"\"\n",
        "        Backpropagating the score through the visited node-action pairs\n",
        "        \"\"\"\n",
        "        for i, (node, action) in enumerate(reversed (node_action_list)):\n",
        "            #Alternating (+/-) score depending on which players turn it is\n",
        "            node.visit(action, score * (-1)**i)\n",
        "\n",
        "\n",
        "    def choose_action(self, node):\n",
        "        \"\"\"\n",
        "        Choose the best action from the current node, based on the default policy and utility function\n",
        "        :return argmax(q_sa + u_sa)\n",
        "        \"\"\"\n",
        "        actions = node.children.keys()\n",
        "        n_s = node.visit_count\n",
        "        node_state = node.state\n",
        "\n",
        "        _, player = node_state\n",
        " \n",
        "        return max(actions, key=lambda a: self.score_move(node=node, state=node_state, n_s=n_s, action=a))\n",
        "    \n",
        "    def score_move(self, node, state, n_s, action):\n",
        "        \"\"\"\n",
        "        Score Q-values\n",
        "        :return (q_sa + u_sa)\n",
        "        \"\"\"\n",
        "        # visit count\n",
        "        n_sa = node.edges[action][1]\n",
        "\n",
        "        u_sa = self.utility_function(n_s=n_s, n_sa=n_sa, c=self.c)\n",
        "        q_sa = node.get_q(action)           \n",
        "        return q_sa + u_sa\n",
        "   \n",
        "\n",
        "    def get_prob_distribution(self, node):\n",
        "        \"\"\"\n",
        "        Returning normalized probability distribution for possible actions, based on the visitcounts\n",
        "        Shape of prob_distribution equals board shape\n",
        "        \"\"\"\n",
        "        visit_count = np.zeros_like(self.state[0])\n",
        "        w = np.zeros_like(self.state[0])\n",
        "        for key in node.edges.keys():\n",
        "            move = key[0]\n",
        "            visit_count[move] = node.edges[key][1]\n",
        "            w[move] = node.edges[key][0]\n",
        "        \n",
        "        prob_distribution = visit_count/sum(sum(visit_count))\n",
        "        return prob_distribution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Node:\n",
        "    \"\"\"\n",
        "    Nodes in the Monte Carlo Tree\n",
        "    \n",
        "    Hashing nodes to static dictionary memo, in order to reuse them at later states\n",
        "    \"\"\"\n",
        "    memo = dict()\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_memo():\n",
        "        memo = dict()\n",
        "\n",
        "    def __init__(self, state, state_manager):\n",
        "        self.state = state\n",
        "\n",
        "        #children = {action: Node(child_state)}\n",
        "        #action = ((row, col), player)\n",
        "        self.children = {}\n",
        "        #edges = {action: (value, visit_count)}\n",
        "        self.edges = {}\n",
        "        #self.set_child_states(state_manager.get_legal_actions(state), state_manager)\n",
        "        self.visit_count = 1\n",
        "\n",
        "\n",
        "    def get_child(self, action):\n",
        "        return self.children[action]\n",
        "\n",
        "    def set_child_states(self, actions, state_manager):\n",
        "        \"\"\"\n",
        "        Used when Expanding nodes.\n",
        "        If any child nodes have bbeen generated at an earlier stage in the game, they are reloaded\n",
        "        \"\"\"\n",
        "        if not actions:\n",
        "            return\n",
        "        for action in actions:\n",
        "            child_state = state_manager.get_child_state(self.state, action)\n",
        "            hash_ = hash(child_state[0].tobytes())\n",
        "            if hash_ in Node.memo.keys():\n",
        "                self.children[action] = Node.memo.get(hash_)\n",
        "            else:\n",
        "                node = Node(child_state, state_manager)\n",
        "                Node.memo[hash_] = node\n",
        "                self.children[action] = node\n",
        "            \n",
        "            #Set edge_visits initially to 1 to avoid divi\n",
        "            self.edges[action] = (0 ,1)\n",
        "\n",
        "    def is_final_state(self):\n",
        "        if len(self.children) == 0:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def visit(self, action, value):\n",
        "        self.visit_count +=1\n",
        "        edge_value, edge_visits = self.edges[action]\n",
        "        self.edges[action] = (edge_value + value, edge_visits + 1) \n",
        "    \n",
        "    def get_q(self, action):\n",
        "        edge_value, edge_visits = self.edges[action]\n",
        "        return (1/edge_visits)*edge_value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "One-hot encode the board before entering it into the NN\n",
        "\n",
        "Create a convolutionary 2D network with 2d input;\n",
        "    1. One Hot Encoded board for player 1\n",
        "    2. One Hot Encoded board for player 2\n",
        "\"\"\"\n",
        "\n",
        "class ANET:\n",
        "    def __init__(self, epochs):\n",
        "        self.model = Sequential()\n",
        "        self.epochs = epochs\n",
        "        pass\n",
        "        \n",
        "    \n",
        "    def initialize(self, board_shape, filters=[32,64, 64], kernel_sizes=[(3,3), (2,2), (1,1)], dense_shape=[15, 30], \n",
        "                   activation='relu', optimizer='SGD', padding='same', lossfunction='categorical_crossentropy'):\n",
        "        \"\"\"\n",
        "        Initializing the Neural Network Model\n",
        "\n",
        "        :param board_shape: (nRows, nCols)\n",
        "        :param filters: filter sizes in Conv2D\n",
        "        :param kernel_sizes: kernel_sizes in Conv2D\n",
        "        :param dense_shape: shape of dense layers\n",
        "        :param activation_function: string identifier of built-in Keras activation function\n",
        "        :param optimizer: string identifier of built-in Keras activation function\n",
        "        :param padding: string identifier of built-in Keras padding function\n",
        "        :param lossfunction string identifier of built-in Keras loss function\n",
        "            \n",
        "        input_shape = (nRows, nCols, nDims) nDims=2 because we have one perspective of the board from each player\n",
        "        \"\"\" \n",
        "\n",
        "        input_shape = (board_shape[0], board_shape[1], 2)\n",
        "\n",
        "        #Convolutional layers\n",
        "        for i in range (len(filters)):\n",
        "            self.model.add(Conv2D(filters[i], kernel_size=kernel_sizes[i], activation=activation, padding=padding))\n",
        "\n",
        "        self.model.add(Flatten())\n",
        "        \n",
        "        #Dense layers\n",
        "        for i in range(len(dense_shape)):\n",
        "            self.model.add(Dense(dense_shape[i] , activation=activation))\n",
        "\n",
        "        self.model.add(Dense(board_shape[0]*board_shape[1], activation='softmax'))\n",
        "        self.model.compile(loss=lossfunction, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    def get_action(self, state, legal_actions, stochastic=False):\n",
        "        board, player = state\n",
        "        board_shape = board.shape\n",
        "       \n",
        "        ohe_boards = np.array([self.one_hot_encode(state)])\n",
        "        stack = tf.stack(ohe_boards)\n",
        "        \n",
        "        prediction = self.model(stack).numpy()\n",
        "        prediction = self.pred_decode(prediction.reshape(board_shape), player)\n",
        "        \n",
        "        #Used for playing games\n",
        "        if stochastic:\n",
        "            prediction = {a:prediction[a[0]] for a in legal_actions}\n",
        "            action = (random.choices(population=list(prediction.keys()), weights=list(prediction.values()), k=1)[0])\n",
        "        \n",
        "        else:\n",
        "            max_pred = 0\n",
        "            move = None\n",
        "            for a in legal_actions:\n",
        "                if prediction[a[0]] > max_pred:\n",
        "                    max_pred = prediction[a[0]]\n",
        "                    move = a[0]\n",
        "            player = legal_actions[0][1]\n",
        "            action = (move, player)\n",
        "            \n",
        "        return action\n",
        "        \n",
        "    def train(self, minibatch):\n",
        "        \"\"\"\n",
        "        :param minibatch = (state, prob_distribution)\n",
        "\n",
        "        fitting the states and prob_distributions from training\n",
        "        \"\"\"\n",
        "        \n",
        "        X = [self.one_hot_encode(s) for s, _ in minibatch]\n",
        "        y = [self.pred_decode(p, s[1]).flatten() for s, p in minibatch]\n",
        "\n",
        "        self.model.fit(np.stack(X), np.stack(y), batch_size=len(minibatch), epochs=self.epochs)\n",
        "\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        self.model.save(filepath=filepath)\n",
        "    \n",
        "    def load_model(self, filepath):\n",
        "        self.model = keras.models.load_model(filepath=filepath)\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    \"\"\"\n",
        "    Encoding and decoding of NN input and output\n",
        "    \"\"\"\n",
        "    def pred_decode(self, prediction, player):\n",
        "        if player == 1:\n",
        "            return prediction\n",
        "        else:\n",
        "            return prediction.T\n",
        "        \n",
        "    def one_hot_encode(self, state):\n",
        "        \"\"\"\n",
        "        :param state on format tuple(game_board, player)\n",
        "        :return one-hot-encoded state on the form [[bin 2D-array of the current players pegs],\n",
        "        [bin 2D-array of the other players pegs]]\n",
        " \n",
        "        \"\"\"\n",
        "        board, player = state\n",
        "        board_size = len(board[0])\n",
        "        p1_board = np.where(board == 1, 1, 0)\n",
        "        p2_board = np.where(board == 2, 1, 0)\n",
        "        \n",
        "        ohe = np.zeros(shape=(board_size, board_size, 2))        \n",
        "        for i in range(board_size):\n",
        "            for j in  range(board_size):\n",
        "                if player == 1:\n",
        "                    ohe[i,j] = [p1_board[i,j], p2_board[i,j]]\n",
        "                else:\n",
        "                    ohe[i,j] = [p2_board.T[i,j], p1_board.T[i,j]]\n",
        "\n",
        "        return ohe"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actor\n",
        "For training the ANET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Actor:\n",
        "    \"\"\"\n",
        "    Used for training the ANET\n",
        "    \"\"\"\n",
        "    def __init__(self, state_manager, anet, rbuf_lim=100, c=1.5):\n",
        "        self.sm = state_manager\n",
        "        self.anet = anet\n",
        "        self.RBUF = deque([], rbuf_lim)\n",
        "        self.anet_parameters = []\n",
        "        self.c = c\n",
        "\n",
        "    def train_anet(self, n_games, n_sim, utility_function, save_interval, minibatch_size, filepath):\n",
        "        #self.clear_RBUF()\n",
        "        for i in range(1, n_games+1):\n",
        "            print(\"Game\", i)\n",
        "            #Clearing the Node cache\n",
        "            Node.clear_memo()\n",
        "\n",
        "            self.play_game(save_interval, n_sim, utility_function, minibatch_size)\n",
        "\n",
        "            #if ga modulo is == 0: Save ANET’s current parameters for later use in tournament play.\n",
        "            if np.mod(i, save_interval) == 0:\n",
        "                path=filepath +'/model_'+ str(i)\n",
        "                self.anet.save_model(path)\n",
        "                pickle.dump( self.RBUF, open( \"rbuf.p\", \"wb\" ))\n",
        "        path=filepath + '/final_model'\n",
        "        self.anet.save_model(path)\n",
        "        print(\"Finished training\")\n",
        "\n",
        "\n",
        "    def play_game(self, save_interval, n_sim, utility_function, minibatch_size):\n",
        "        board = self.sm.get_initial_board()\n",
        "        player = 1\n",
        "        state = (board, player)\n",
        "\n",
        "        \n",
        "        root_node = None\n",
        "        winner = None\n",
        "\n",
        "        while len(self.sm.get_legal_actions(state)) > 0 and winner is None:\n",
        "            mcts = MCTS(self.sm, n_sim, self.anet, uct, state, root_node=root_node, c=self.c)\n",
        "            best_move, prob_distribution, child_node = mcts.choose_move()\n",
        "\n",
        "            self.RBUF.append((state, prob_distribution))\n",
        "            \n",
        "            child_state = self.sm.get_child_state(state, best_move)\n",
        "\n",
        "            if self.sm.check_if_winning(child_state):\n",
        "                _, next_player = child_state\n",
        "                winner = change_player(next_player)\n",
        "        \n",
        "            # set state as the board state and the other player\n",
        "            state = child_state\n",
        "            root_node = child_node\n",
        "\n",
        "        print(\"Winner: \", winner)\n",
        "        #Train ANET on a random minibatch of cases from RBUF\n",
        "        if len(self.RBUF) <= minibatch_size:\n",
        "            minibatch = random.sample(self.RBUF, len(self.RBUF))\n",
        "        else:\n",
        "            minibatch = random.sample(self.RBUF, minibatch_size)\n",
        "        self.anet.train(minibatch)\n",
        "\n",
        "    \n",
        "    def clear_RBUF(self):\n",
        "        self.RBUF = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# board_size = 4\n",
        "# sm = State_Manager(board_size)\n",
        "# anet = ANET(epochs=10)\n",
        "# anet.initialize(board_shape=(board_size, board_size))\n",
        "\n",
        "# actor = Actor(sm, anet, rbuf_lim=100)\n",
        "# actor.train_anet(n_games=1, n_sim=75, utility_function=uct, save_interval=1, minibatch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TOPP:\n",
        "    \n",
        "    def __init__(self, board_size, model_paths, G):\n",
        "        self.board_size = board_size\n",
        "        self.anets = [self.init_anet(path) for path in model_paths]\n",
        "        self.G = G\n",
        "\n",
        "    def init_anet(self, path):\n",
        "        anet = ANET(epochs=None)\n",
        "        anet.load_model(path)\n",
        "        return anet\n",
        "\n",
        "    def run_tournament(self, visualize=False):\n",
        "        # create pairs of matches between all the players\n",
        "        match_pairs = combinations(range(len(self.anets)), 2)\n",
        "        score_table = np.zeros_like(self.anets)\n",
        "\n",
        "        sm = State_Manager(self.board_size)\n",
        "        \n",
        "        for pair in match_pairs:\n",
        "            p1, p2 = pair\n",
        "\n",
        "            def p1_move(state, root_node=None):\n",
        "                legal_actions = sm.get_legal_actions(state)\n",
        "                anet_action = self.anets[p1].get_action(state, legal_actions, stochastic=True)\n",
        "                return anet_action, None\n",
        "\n",
        "            def p2_move(state, root_node=None):\n",
        "                legal_actions = sm.get_legal_actions(state)\n",
        "                anet_action = self.anets[p2].get_action(state, legal_actions, stochastic=True)\n",
        "                return anet_action, None\n",
        "            \n",
        "            # Play match G times\n",
        "            for i in range(self.G):\n",
        "                winner, state_history = play_game(sm, p1_move, p2_move)\n",
        "                if visualize:\n",
        "                    hv = HexVisualizer()\n",
        "                    hv.render_gif(state_history, f'out/p{p1}-VS-p{p2}-{i}.gif')\n",
        "                if winner == 1:\n",
        "                    score_table[p1] += 1\n",
        "                elif winner == 2:\n",
        "                    score_table[p2] += 1\n",
        "                else:\n",
        "                    raise Exception('PLAYER 1 OR PLAYER 2 SHOULD HAVE WON!')\n",
        "        return score_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_paths = ['NN_test/model_20', 'NN_test/model_10']\n",
        "# topp = TOPP(PARAMS.board_size, model_paths, 100)\n",
        "# scores = topp.run_tournament()\n",
        "# print(scores)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uct(n_s, n_sa, c=1):\n",
        "    return c*np.sqrt((np.log(n_s))/(1 + n_sa))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Game Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GameController:\n",
        "\n",
        "    def __init__(self):\n",
        "        #ANET\n",
        "        self.epochs = 10\n",
        "        self.lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "                initial_learning_rate=1e-2,\n",
        "                decay_steps=10000,\n",
        "                decay_rate=0.9)\n",
        "        self.optimizer = keras.optimizers.SGD(learning_rate=self.lr_schedule)\n",
        "\n",
        "        #Format of Conv2D-layers\n",
        "        self.filters = [32,64, 64]\n",
        "        self.kernel_sizes = [(3,3), (2,2), (1,1)]\n",
        "\n",
        "        #Format of dense layers\n",
        "        self.dense_shape = [15, 30]\n",
        "\n",
        "        self.activation = 'relu'\n",
        "        # self.optimizer = 'SGD'\n",
        "        self.padding = 'same'\n",
        "        self.lossfunction = 'categorical_crossentropy'\n",
        "        self.learning_rate = 0.02\n",
        "\n",
        "        # GENERAL\n",
        "        self.board_size = 5\n",
        "\n",
        "        #TOPP   \n",
        "            # G=Number of games between each model\n",
        "            # M = Number of saves // number of models\n",
        "        self.G = 10\n",
        "        self.M = 1\n",
        "\n",
        "        self.visualize_TOPP = True\n",
        "        self.visualize_training = False\n",
        "\n",
        "        # Actor/Training ANET\n",
        "        self.rbuf_lim = 100\n",
        "        self.c = 1.5\n",
        "        self.n_games = 100\n",
        "        self.n_sim = 100\n",
        "        self.utility_function = uct\n",
        "        self.minibatch_size = 10\n",
        "\n",
        "        self.file_path = 'DemoModel'\n",
        "        \n",
        "\n",
        "    def run_TOPP(self, model_paths=None):\n",
        "        # If no pre-trained model_paths is passed as parameter, train models for TOPP\n",
        "        if model_paths is None:\n",
        "            print(f'----Training ANET ----')\n",
        "            # interval at which models are saved\n",
        "            save_interval = int(self.n_games/self.M)\n",
        "            # list of the paths of the M trained models\n",
        "            model_paths = [ f'{self.file_path}/model_{str(i)}' for i in range(save_interval,self.n_games+1, save_interval)]\n",
        "            self.train_model(save_interval=save_interval)\n",
        "            print(\"----Training Complete ----\")\n",
        "\n",
        "        topp = TOPP(self.board_size, model_paths, self.G)\n",
        "        scores = topp.run_tournament(visualize=self.visualize_TOPP)\n",
        "\n",
        "        # Visualize the scores of the TOPP\n",
        "        player_names = [x.split('/')[-1] for x in model_paths]\n",
        "        player_scores = zip(player_names, scores)\n",
        "        print('==== TOPP RESULTS ====')\n",
        "        for player, score in player_scores:\n",
        "            print(f'  {player}: {score} WINS')\n",
        "        print('======================')\n",
        "\n",
        "    def train_model(self, save_interval):\n",
        "        statemanager = State_Manager(self.board_size)\n",
        "        anet = ANET(self.epochs)\n",
        "        anet.initialize(board_shape=(self.board_size, self.board_size), filters=self.filters,\n",
        "                        kernel_sizes=self.kernel_sizes, dense_shape=self.dense_shape, activation=self.activation,\n",
        "                        optimizer=self.optimizer, padding=self.padding, lossfunction=self.lossfunction)\n",
        "\n",
        "        actor = Actor(state_manager=statemanager, anet=anet, rbuf_lim=self.rbuf_lim, c=self.c)\n",
        "        actor.train_anet(self.n_games, self.n_sim, self.utility_function, save_interval,\n",
        "                         self.minibatch_size, self.file_path)\n",
        "        if self.visualize_training:\n",
        "            state_history = [rbuf[0] for rbuf in actor.RBUF]\n",
        "            hv = HexVisualizer()\n",
        "            hv.render_gif(state_history, 'out/rbuf.gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "out/p0-VS-p1-0.gif\n",
            "out/p0-VS-p1-1.gif\n",
            "out/p0-VS-p1-2.gif\n",
            "out/p0-VS-p1-3.gif\n",
            "out/p0-VS-p1-4.gif\n",
            "out/p0-VS-p1-5.gif\n",
            "out/p0-VS-p1-6.gif\n",
            "out/p0-VS-p1-7.gif\n",
            "out/p0-VS-p1-8.gif\n",
            "out/p0-VS-p1-9.gif\n",
            "==== TOPP RESULTS ====\n",
            "  model_10: 1 WINS\n",
            "  model_1000: 9 WINS\n",
            "======================\n"
          ]
        }
      ],
      "source": [
        "gc = GameController()\n",
        "gc.run_TOPP(model_paths=['ProModel-5x5/model_10', 'ProModel-5x5/model_1000'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gc = GameController()\n",
        "# gc.run_TOPP(model_paths=['ProModel/model_10', 'ProModel/model_100', 'ProModel/model_1000'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a6dc4257fd192e60ab8d993d4ee2aab78349d12e62094a77321387ba51138de8"
    },
    "kernelspec": {
      "display_name": "Python 3.8.6 64-bit ('3.8.6': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
